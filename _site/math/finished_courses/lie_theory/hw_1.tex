% --------------------------------------------------------------
% Andrew Tindall
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,enumitem, tikz-cd}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\e}{\varepsilon}
\newcommand{\bs}{\backslash}
\newcommand{\PGL}{\text{PGL}}
\newcommand{\Sp}{\text{Sp}}
\newcommand{\tr}{\text{tr}}
\newcommand{\Lie}{\text{Lie}}
\newcommand{\rec}[1]{\frac{1}{#1}}
\newcommand{\toinf}{\rightarrow \infty}


\theoremstyle{definition}
\newtheorem{proofpart}{Part}
\newtheorem{theorem}{Theorem}
\makeatletter
\@addtoreset{proofpart}{theorem}
\makeatother


\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
 
\begin{document}
 
%\renewcommand{\qedsymbol}{\filledbox}
 
\title{Homework 1}
\author{Andrew Tindall\\
Lie Theory}
 
\maketitle
\begin{section}{Book Problems}
\begin{problem}{1}
	Br\"ocker \& tom Dieck, 1.1.16.5: Show that a discrete normal subgroup of a connected Lie group must be contained in the center of the group.
\end{problem}
\begin{proof}
	Let $H$ be a discrete normal subgroup of a connected Lie group $G$, and let $h_1 \in H$ and $g \in G$ be arbitrary. Because $H$ is normal, we know that $gh_1g^{-1} = h_2$ for some $h_2 \in H$. Now, $G$ is connected, so there exists a continuous path $f: [0,1] \to G$ such that $f(0) = e_G$, and $f(1) = g$. 
	\par As $G$ is a Lie group, multiplication and inversion are continuous, so we also know that the map $\varphi: x \mapsto xh_1x^{-1}$ is continuous, and composing these two maps we see that $\varphi \circ f : [0,1] \to G$ is a continuous path such that $(\varphi \circ f )(0) = h_1$ and $(\varphi \circ f)(1) = gh_1g^{-1} = h_2$. 
	\par Now, $H$ was assumed to be discrete, so the existence of a path between two elements of $H$ implies that the elements are equal: $h_1 = gh_1g^{-1}$. This shows that $h_1g = gh_1$. $g \in G$ and $h \in H$ were arbitrary, so every element of $H$ must commute with every element of $G$, meaning that $H$ is contained in the center of $G$.
\end{proof}
\begin{problem}{2}
	Br\"ocker \& tom Dieck, 1.1.16.8: Let $D \subset \text{SL}(n, \R)$ be the group of upper triangular matrices with positive elements on the diagonal. Show that the map 
	\[D \times \text{O}(n) \to \text{GL}(n, \R), \qquad (A,C)\mapsto A \cdot C\]
	is a diffeomorphism. Conclude that $\text{GL}(n, \R)$ is diffeomorphic to $\text{O}(n) \times \R^{n(n+1)/2}$.
	\par Show in the same way that $B \times \text{U}(n) \to \text{GL}(n, \C)$ is a diffeomorphism, where $B$ is the group of triangular complex matrices with positive real diagonals. Thus $\text{GL}(n, \C) \cong \text{U}(n) \times \R^{n \times n}$ as a manifold. Also, show that $\text{SL}(n,\R) \cong \text{SO}(n) \times \R^{(1/2)n\cdot (n+1)- 1}$ as manifolds, and in particular $\text{SL}(2, \R) \cong S^1 \times \R^2$.
\end{problem}
\begin{proof}
	\par	I did this proof with multiplication in the opposite direction - $\text{O}(n) \times D$, instead of the other way around. I'm not sure why I couldn't adapt it to the other way, but it does still imply that the two spaces are diffeomorphic.
\par	It is true that the map $\text{O}(n) \times D \to \text{GL}(n, \R)$ induced by multiplication is continuous and differentiable, as it is polynomial, componentwise, in the entries of the matrices in $D$ and in $\text{O}(n)$. We now construct an inverse map.
	\par By identifying an $n\times n$ matrix as an ordered set of $n$ column vectors, the space $\text{GL}(n,\R)$ may be thought of as the set of all ordered sets $\left\{ v_1, \dots v_n \right\}$ of $n$ linearly independent vectors in $\R^n$. Similarly, $\text{O}(n)$ consists of all ordered sets $\left\{ u_1, \dots u_n \right\}$ of $n$ vectors such that $\langle u_i, uj \rangle$ is equal to $1$ if $i = j$ and zero otherwise. 
	Now, we want to construct a function on a set of $n$ independent vectors. We do so inductively: First, assume we have a function on a set of $m-1$ independent vectors $\left\{ v_1, \dots v_{m-1} \right\}$, $m \leq n$, that gives us a pair of matrices such that
	\[ \begin{bmatrix}
v_1&
v_2&
\dots&
v_{m-1}&
\end{bmatrix} \begin{bmatrix}
			d_{11} & d_{12} & \dots &  d_{1,m-1)}\\
			0 & d_{22} & \dots & d_{m-1} \\
			\vdots & \vdots & \ddots & \vdots \\			
			0 & 0 & \dots & d_{m-1,m-1}
\end{bmatrix}
= \begin{bmatrix}
	u_1 &u_2  &\dots  &u_{m-1}
\end{bmatrix}\]
(The entries of $U$ and $V$ are written as column vectors), where $d_{ii} > 0$, the vectors $u_i \in \R^{n}$ have norm $1$ and are pairwise orthogonal with respect to the inner product $\langle \cdot, \cdot\rangle$, and the entries of the two matrices $D$ and $U$ are locally differentiable functions of the components of the vectors $v_1, \dots v_{m-1}$. Now, adjoin a vector $v_m$ such that the set $\left\{ v_1, \dots v_m \right\}$ is linearly independent. 
\par We construct $u_m$ and the column $d_{*,  m}$ following the $m$th step of the Graham-Schmidt orthogonalization process. Let the vector $u'_m$ be defined as 
\[ u'_m = v_m - \left( \sum_{i = 1}^{m-1} \langle v_m, u_i\rangle v_m \right)\]
\par As the entries of the $u_i$ are differentiable functions of the components of the $v_i$, and $\langle \cdot, \cdot\rangle$ is a differentiable function, this defines the components of $u'_m$ as differentiable functions of the entries of $v_1, \dots v_m$. Finally, let $a = \sqrt{\langle u'_m, u'_m\rangle}$. This is well-defined and differentiable, as $u'_m$ must be nonzero, thanks to the linear independence of the set $\left\{ v_1, \dots v_n \right\}$. In particular, $a$ is positive.
\par Define $d_{i,m} = -\frac{\langle v_m, u_i\rangle}{a} $ for $1 \leq i \leq m$, and  let $d_{m,m} = \frac{1}{a}$. Then, by the Graham-Schmidt process, we see that 
\[ \begin{bmatrix}
		v_1 & v_2 & \dots & v_{m-1} & v_m
\end{bmatrix}
\begin{bmatrix}
	d_{11} & d_{12} & \dots & d_{1, m-1} & -\frac{\langle v_m, u_1\rangle}{a}\\
	0 & d_{22} & \dots & d_{2, m-1} &-\frac{\langle v_m, u_2\rangle}{a} \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & \dots & d_{m-1, m-1} & -\frac{\langle v_m, u_{m-1}\rangle}{a}\\
	0 & 0 & \dots & 0 & \frac{1}{a}
\end{bmatrix} = \begin{bmatrix}
	u_1 &  u_2 & \dots & u_{m-1} & u_{m}
\end{bmatrix}\]
gives us a transformation of $\left\{ v_i \right\}$ into an orthonormal set $\left\{ u_i \right\}$.
\par To finish our induction, we see that the base case is a single nonzero vector $v_1$, in which case our decomposition is 
\[\begin{bmatrix}
	v_1
\end{bmatrix} \begin{bmatrix}
	\frac{1}{\sqrt{\langle v_1, v_1\rangle}}
\end{bmatrix} = \begin{bmatrix}
u_1
\end{bmatrix}\]
So, by induction, we have an assignment $V \mapsto D, U$ which gives an upper-triangular matrix with positive diagonal $D$ and an orthogonal matrix $U$ for every invertible matrix $V$, such that $VD = U$.
\par Now, because $\text{det}(D) > 0$ and $D$ is upper triangular, $D^{-1}$ exists and is also upper triangular with positive diagonal, and its entries are differentiable with respect to the entries of $D$ and therefore with respect to those of $V$. Now, we see that 
\begin{align*}
	VD &= U\\
	V &= UD^{-1}
\end{align*}
Therefore, the map $V \mapsto (U, D^{-1})$ is a differentiable left-inverse to the multiplication map.
\par We finally show that the multiplication map is one-to-one. Let $(D, U)$ and $(D', U')$ be two pairs of upper-triangular positive-diagonal matrices and orthogonal matrices, so that $DU = D'U'$. We show inductively that each column of $U$ is equal to the corresponding column of  $U'$.
\par First of all, it is clear that the first row of $U$ must be a scalar multiple of the first row of $U$, because $D$ and $D$ are upper triangular:
\[ \begin{bmatrix}
		d_{11} & & *\\
		 & d_{22} & \\
		0  & & \ddots
	\end{bmatrix} \begin{bmatrix}
		u_1 & u_2 & \dots
	\end{bmatrix} = 
\begin{bmatrix}
		d'_{11} & & *\\
		 & d'_{22} & \\
		0  & & \ddots
	\end{bmatrix} \begin{bmatrix}
		u'_1 & u'_2 \dots 
	\end{bmatrix}
\]
Thus it must be that $u_1 = \frac{d'_{11}}{d_{11}}u'_1$. Because $\langle u_1, u_1\rangle = \pm \langle u'_1, u'_1\rangle$, it must be true that $\frac{d'_{11}}{d_{11}} = \pm 1$; since both $d_{11}$ and $d'_{11}$ are positive, we see that $u'_1 = u_1$.
\par Inductively, assume that $u_i = u'_i$ for $1 \leq i \leq m-1$. Then, looking at the $m$th column vector of $DU$, we have 
\[\sum_{i = 1}^m d_{mi}u_i = \sum_{i=1}^{m-1}d'_{mi}u_i + d_{mm}u'_m\]
Because the first $m-1$ terms of both sums are in a space orthogonal to $u'_m$ and $u_m$, it must be true that $d_{mm} u_m = d'_{mm}u'_m$. Again, because $d_{mm}$ and $d'_{mm}$ are positive, we see that $u_m = u'_m$. By induction on $m$, each column of $U$ must be equal to the corresponding column of $U'$. By invertibility of $U$, we see that $DU = D'U \Rightarrow D = D'$. This shows that $DU = D'U'$ implies that $D = D'$ and $U = U'$.
 Therefore, the map $D, U \mapsto D \cdot U$ is one-to-one, and its differentiable left-inverse also forms a differentiable right-inverse. 
 \par This finally shows that the spaces $\text{O}(n) \times D$ and $\text{GL}(n, \R)$ are diffeomorphic. To see that $D$ is itself diffeomorphic to $\R^{n(n+1)/2}$, consider $D$ as a subspace of $M_{n\times n}(\R) \cong \R^{n^2}$, and note that every entry above the diagonal in an upper triangular matrix is arbitrary, and every element in the diagonal can be chosen arbitrarily from $\R_{>0}$. This shows that $D \cong \R^{n(n-1)/2} \times \R_{>0}^n$. Finally, by the diffeomorphism $\text{exp}: \R \cong \R_{>0}$, we see that $D \cong \R^{n(n+1)/2}$. In summary, we can say that the Iwasawa decomposition of $\text{GL}(n)$ is $\text{O}(n) \times \R^{n(n+1)/2}$.
 \par We may adapt this proof to the space $\text{GL}(n, \C)$, with few changes. The Graham-Schmidt process is well-defined on any inner-product space; in particular, we may use $\C^n$ with the inner product $\langle u, v\rangle = u \cdot \bar{v}$. An element of $\text{GL}(n, \C)$ is again an ordered collection of $n$ linearly indpendent vectors in $\C^n$, and the Graham-Schmidt process gives us a decomposition of any element $V \in \text{GL}(n, \C)$ as an upper triangular matrix and a matrix of vectors $u_i \in \C^n$ such that $\langle u_i, u_j\rangle$ is $1$ if $i = j$ and $0$ otherwise:
 \[\begin{bmatrix}
		 v_1 & v_2 & \dots & v_n
	 \end{bmatrix}\begin{bmatrix}
		 d_{11} & d_{12} & \dots & d_{1n}\\
		 0 &  d_{22} & \dots & d_{2n}\\
		 \vdots & \vdots & \ddots & \vdots \\
		 0 & 0 & \dots & d_{nn}
	 \end{bmatrix} = \begin{bmatrix}
		 u_1 & u_2 & \dots & u_n
 \end{bmatrix}\]
 While these matrices are complex, the diagonal entries $d_{ii}$ are again defined as $\frac{1}{\langle u_i, u_i\rangle}$ for some vector $u_i$, which is always positive real number. Again, this is a differentiable two-sided inverse to the multiplication function.
 \par Now, in the complex case, an $n \times n$ matrix composed of pairwise orthogonal vectors, all of norm $1$, is a unitary matrix. Further, the space of upper triangular matrices with positive real entries is diffeomorphic to the space $\C^{n(n-1)/2} \times \R_{>0}^n$, because the entries above the diagonal are arbitrary complex numbers, and those on the diagonal are arbitrary positive reals. Using the diffeomorphisms $\R_{> 0} \cong \R$ and $\C \cong \R^2$, we see that $\text{GL}(n, \C) \cong U(n) \times \R^{n^2}$.
	\par Finally, we look at the subgroup $\text{SL}(n, \R)$ of $\text{GL}(n, \R)$. Under the diffeomorphism defined above, we know that we can decompose any $V \in \text{SL}(n, \R)$ as $D \cdot U$, for $D$ an upper-triangular positive-diagonal matrix, and $U$ orthogonal. Because the determinant of $D$ must be positive, we know that the determinant of $U$ must be as well, which places it in $\text{SO}(n)$. 
	\par Now, since the determinant of $U$ is $1$, and so is $\text{det}(V) = \text{det}(D)\text{det}(U)$, we know that the determinant of the upper triangular matrix $D$ must also be $1$. The determinant of $D$ is the product of the entries on its diagonal, so this reduces the dimension of the selection of its diagonal entries by one - once we know $n-1$ positive entries $\left\{ d_{11}, d_{22}, \dots d_{n-1, n-1} \right\}$, the last entry $d_{nn}$ must be $(d_{11} \cdot d_{22} \cdot \dots \cdot d_{n-1, n-1})^{-1}$. Therefore, the space of such upper-triangular matrices is diffeomorphic to the space $\R^{n(n+1)/2 - 1}$.
	\par Conversely, $\text{det}(DU) = 1$ for any orthogonal matrix $U$ with determinant $1$, and upper triangular matrix with positive diagonal and determinant $1$, meaning that the multiplication map takes such matrices $D, U$ to $\text{SL}(n,\R)$. Therefore, the multiplication map and Graham-Schmidt decomposition are still inverses when restricted to the subspaces $\text{SL}(n, \R)$ and $\text{SO}(n) \times \R^{n(n+1)/2 - 1}$. 
	\par 	So, we see that $\text{SO}(n) \times \R^{n(n+1)/2 - 1}$ is the Iwasawa decomposition of $\text{SL}(n, \R)$. In particular, in the case $n = 2$, the space $\text{SO}(2)$ is simply $S^1$, the one-sphere. Therefore, in this case, $\text{SL}(2, \R) \cong S^1 \times \R^2$.
\end{proof}
\begin{problem}{3}
	Let $P \subset \text{GL}(n, \R)$ be the set of positive-definite symmetric matrices. Show that multiplication induces a bijection $P \times \text{O}(n) \to \text{GL}(n, \R)$.
	\par Let $H \subset \text{GL}(n, \C)$ be the set of positive-definite Hermitian matrices. Show that multiplication induces a bijection $H \times \text{U}(n) \to \text{GL}(n, \C)$.
\end{problem}
\begin{proof}
	We first see that for any positive-definite matrix $P$, and orthogonal matrix $U$, we have that $PU$ is invertible, because $P$ and $U$ are, and $PU$ is therefore an element of $\text{GL}(n)$. On the other hand, let $A \in \text{GL}(n, \R)$: we show that there is a unique decomposition $PU$ of $A$ into a positive-definite matrix and an orthogonal matrix.
	\par For any any invertible $A$, it is true that $A \cdot A^t$ is a positive definite matrix, because for any nonzero $x$, 
	\[x(AA^t)x = (A^tx)^t(A^tx) = \left \lVert { A^t x } \right \lVert^{2} > 0\]
	Further, any positive definite matrix is the square of a unique positive definite matrix \cite{lax}; say $AA^t = B^2$ for $B \in P$.  We want to show that the decomposition
	\[B(B^{-1}A)\]
	gives a right inverse $A \mapsto (B, B^{-1} A)$ to the multiplication map. We need only show that $B^{-1}A$ is orthogonal: That $B^{-1}A(B^{-1}A)^t = I$. Because $B^{-1}$ is positive-definite, we know that $(B^{-1})^t = B^{-1}$. Calculating:
\begin{align*}
	B^{-1}A(B^{-1}A)^t &= B^{-1}AA^t(B^{-1})^t\\
	&= B^{-1}B^2B^{-1}\\
	&= I
\end{align*}
Therefore, we have shown a right-inverse to the multiplication map from $P \times \text{O}(n, \R) \to \text{GL}(n, \R)$, showing it is surjective.
\par Finally, we see that the multiplication map is one-to-one. Let $P, P'$ be positive definite matrices, and $U, U'$ orthogonal matrices, so that $PU = P'U'$. We see that $P = P'U'U^t$, and therefore that $P'^{-1}P = UU'^{-1}$. And, because the inverse of a positive definite matrix is positive definite, we see that $UU'$ is an orthogonal, symmetric, positive definite matrix. The only such matrix is the identity, so $U' = U^{-1} = U$. Since $PP'^{-1} = I$, this also shows that $P = P'$, and so the multiplication map is indeed one-to-one. Therefore, we can put the two sets $P \times \text{O}(n)$ and $\text{GL}(n, \R)$ in bijection.
	\par We now give a similar proof in the complex case. Let $H$ be the space of all positive-definite Hermitian matrices. The multiplication map takes pairs of matrices in $H \times \text{U}(n)$ to invertible complex matrices, so we have a well-defined map $H \times \text{U}(n) \to \text{GL}(n, \C)$. 
	\par Conversely, let $A \in \text{GL}(n, \C)$ be an arbitrary invertible complex matrix. Then $AA^\dagger$ is a Hermitian matrix, and therefore has a unique Hermitian square root - say $AA^\dagger = B^2$. We want to show that $B^{-1}A$ is unitary, which would show that $A \mapsto (B, B^{-1}A)$ is a right inverse to the multiplication map. It is, as we see:
	\begin{align*}
		B^{-1}A(B^{-1}A)^\dagger &= B^{-1}AA^{\dagger}(B^{-1})^\dagger\\
		&= B^{-1}B^2 B^{-1}\\
		&= I
	\end{align*}
	Therefore, the multiplication map is surjective.
	\par Finally, we show that the multiplication map is injective. Let $(P, U)$ and $(P', U')$ be two pairs of Hermitian and unitary matrices, such that $PU = P'U'$. Then $P'^{-1}P = U'U^{\dagger}$, showing that $UU^{\dagger}$ is both hermitian and unitary; again, the only such matrix is the identity. This shows that $U' = U$, and therefore $P' = P$, and we conclude that the multiplication map is a bijection.
\end{proof}
\begin{problem}{4}
	Br\"ocker \& tom Dieck, 1.1.16.12.  Show:
	\begin{enumerate}[label=(\roman*)]
		\item $\text{O}(2n + 1) \cong \text{SO}(2n + 1) \times \Z/2$ as groups, and
		\item $\text{O}(2n) \cong \text{SO}(2n) \times \Z/2$ and $\text{U}(n) \cong \text{SU}(n) \times S^1$ as manifolds. 
	\end{enumerate}
	In case (ii) describe the multiplication $\text{SO}(2n) \times \Z/2$ inherits from the group $\text{O}(2n)$ (semidirect product).
	\par There is a surjective homomorphism 
	\[ S^1 \times \text{SU}(n) \to \text{U}(n), \qquad (\zeta, A) \mapsto \zeta \cdot A\]
\end{problem}
\begin{proof}
	\begin{enumerate}[label=(\roman*)]
		\item  Let $\varphi: \text{O}(2n + 1) \to \text{SO}(2n + 1) \times \Z/2$ be defined by taking a matrix $A$ with determinant $1$ to $(A, 0)$ and a matrix $B$ with determinant $-1$ to $(-B, 1)$. This is well-defined, since for an odd-dimensional space, $\text{det}(-A) = -\text{det}(A)$. It also defines a homomorphism - if the determinant of $A_1$ is $1$ and that of $A_2$ is $-1$, then $\text{det}(A_1A_2) = -1$, and 
			\begin{align*}
				 \varphi(A_1A_2) &= (A_1A_2, 1) \\
				 &= (A_1, 0) \cdot (A_2, 1)\\
				 &= \varphi(A_1)\varphi(A_2)
			\end{align*}	
			And similarly for the other 3 cases. We also have an inverse homomorphism, which takes $(A, 0)$ to $A$, and $(B, 1)$ to $-B$. Therefore, these two groups are isomorphic.
		\item The same map cannot be constructed in the even-dimensional case, because $\text{det}(-A) \neq -\text{det}(A)$ - in particular, the center of $\text{O}(2n)$ has two elements, $\pm I$, and the center of $\text{SO}(2n) \times \Z/ 2$ has four: $(\pm I, 0)$ and $(\pm I, 1)$. 
			\par However, these two spaces are still diffeomorphic as manifolds, under a map such as the following: Let $E$ be the diagonal matrix with a $-1$ in the top left spot and $1$s along the rest of the diagonal; then $\text{det}(E) = -1$ and $E \in \text{O}(2n)$. For any matrix $A \in \text{O}(2n)$, if $\text{det}(A) = 1$, then define $\varphi(A) = (A, 0)$, and if $\text{det}(A) = -1$, then define $\varphi(A) = (EA, 1)$. 
			\par Both $\text{O}(2n)$ and $\text{SO}(2n)$ are separated manifolds with two components, and this map $\varphi$, restricted to one component of $\text{O}(2n)$, is a diffeomorphism onto one component of $\text{SO}(2n)$, and a diffeomorphism of the other components as well. 
			\par This diffeomorphism induces a semidirect product structure on $\text{SO}(n, \R) \times \Z/2$: Because $\text{SO}(n, \R)$ is a normal subgroup of $\text{O}(n, \R)$, and $\Z/2$ can be embedded as a subgroup of $\text{O}(n, \R)$, specifically the group $\left\{ I, E \right\}$, we can construct the semidirect product $\text{SO}(n,\R) \rtimes \Z/2$. The group law is given by $(A, x) \cdot (B, y) = (AE^xB, x+y)$ -i.e. $(A, 1) \cdot (B, 0) = (AEB, 1)$, while $(A, 0) \cdot (B, 1) = (AB, 1)$. The inverse of an element $(A,x)$ is given by $(E^xA^{-1}, x)$.
				\par Both $\text{SO}(n, \R)$ and $\Z/2$ are embedded in this semidirect product as subgroups; $\text{SO}(n, \R)$ by the map $A \mapsto (A, 0)$, and $\Z/2$ by the map $0 \mapsto (I, 0)$ and $1 \mapsto (E, 1)$.
				\par Now, we look at the complex case : the group $\text{U}(n)$ may be mapped diffeomorphically to $\text{SU}(n) \times S^1$. For any unitary matrix $A$, $\text{det}(A) = \zeta$, for $\zeta$ some complex number with absolute value $1$. For any $\zeta$, define the $n \times n$ complex matrix $F_\zeta$ as the diagonal matrix where the top left entry is $\zeta^{-1}$. Then $F_\zeta \in \text{U}(n)$, and we may define a map $\varphi: \text{U}(n) \to \text{SU}(n) \times S^1$ by sending $A \in \text{U}(n)$ to 
			\[\varphi(A) = (F_\zeta A, \zeta),\]
			where $\zeta$ is the determinant of $A$. Because the determinant is a differentiable function of the entries of $A$, it is clear that the function $A \mapsto \zeta$ is differentiable, and therefore that the map $A \mapsto F_\zeta$ is also differentiable. Finally, the determinant of $F_\zeta A$ is $\zeta^{-1}\zeta = 1$, making $\varphi$ a well-defined differentiable map from $\text{U}(n)\to \text{SU}(n) \times S^1$, where $S^1$ is viewed as the space of complex numbers with absolute value $1$.
			\par The inverse function to $\varphi$ takes a pair $(B, \xi) $ to the matrix $F_{\xi^{-1}}B$, which has determinant $\xi$ and is thus an element of $\text{U}(n)$. This is continuous in the argument $\xi$, as well as the entries of the matrix $B$, and it is an inverse to $\varphi$, so together they form a diffeomorphism of the two groups, $\text{U}(n)$ and $\text{SU}(n) \times S^1$.
	\end{enumerate}
	\par There is also a surjective homomorphism $S^1 \times \text{SU}(n) \to \text{U}(n)$, which takes $(\zeta, A)$ to $\zeta \times A$. It is surjective because, given $B \in \text{U}(n)$, and $\xi = \text{det}(B)$, we see that  $B$ is the image of the element $(\xi^{1/n}, \xi^{-1/n}B)$. This follows from the fact that, in a space of $n\times n$ matrices, the value of $\text{det}(xA)$ is $x^n\text{det}(A)$.
	\par The kernel of this map is the set of all pairs $(\zeta, A)$ such that $\zeta A = I$. We see that $A$ must be a scalar multiple of the identity element, $\zeta^{-1}I$. Because $A \in \text{SU}(n)$, it must also be true that $(\zeta^{-1})^{n} = 1$, i.e. that $\zeta$ is an $n$th root of unity. The subgroup of such matrices is abelian, because it consists of multiples of the identity matrix, and it is generated by the element $((\xi^{-1}, \xi A)$, where $\xi$ is any primitive $n$th root of unity, say $\xi = e^{2\pi i / n}$. Any $n$th root of unity may be expressed as $\xi^k$, where $0 \leq k \leq n-1$. Since this subgroup is finite, and generated by one element, it is the cyclic group of order $n$.
\end{proof}
\begin{problem}{5}
	Br\"ocker \& tom Dieck, I.3.13.1: Show that the exponential map of the group $\text{SL}(2, \R)$ is not surjective. What values can the trace $\text{Tr exp}(A)$ take, if $A \in \mathfrak{sl}(2,\R)$? Calculate the image of the exponential map.	
\end{problem}
\begin{proof}
	We know that the lie algebra $\mathfrak{sl}(2,\R)$ is the space of all matrices with trace $0$. If $T$ is such a matrix, it either has two distinct eigenspaces corresponding to eigenvalues $\lambda_1, -\lambda_1$, or has one generalized eigenspace corresponding to the single eigenvalue $\lambda_1$, which is necessarily $0$. Therefore $T$ has Jordan decomposition $J = PTP^{-1}$ of one of two forms:
	\[J = \begin{bmatrix}
		\lambda_1 & 0\\
		0 & -\lambda_1
	\end{bmatrix}\quad\text{or}\quad \begin{bmatrix}
	0 & 1\\
	0 & 0
\end{bmatrix}\]
Then, exponentiating $T$, we see that 
\[\text{exp}(T) = \text{exp}(P^{-1}JP) = P^{-1}\begin{bmatrix}
		e^{\lambda_1} & 0\\
		0 & e^{-\lambda_1}
	\end{bmatrix}P\quad\text{or}\quad P^{-1}\begin{bmatrix}
	1 & 1\\
	0 & 1
\end{bmatrix}P.\]
The trace of any matrix of the first form is $e^{\lambda_1} + e^{-\lambda_1}$, which by convexity of the exponential function is always greater than or equal to $2e^{(\lambda_1 - \lambda_1)/2} = 2$, and the trace of the second matrix is $2$. Thus the exponential map can only map to elements of $\text{SL}(2, \R)$ with trace $\geq 2$. There are many elements which are not in this image; for instance $\begin{bmatrix}	-1 & 0 \\ 0 & -1 \end{bmatrix}$.
\par On the other hand, any element of $\text{SL}(2,\R)$ which has trace greater than or equal to $2$ is in the image of the exponential map. Let $A \in \text{SL}(2, \R)$ be arbitrary. The Jordan decomposition of $A$, $J = PAP^{-1}$, must have one of two forms:
\[ J = \begin{bmatrix}
	\lambda_1 & 0 \\
	0 & \lambda_1^{-1}
\end{bmatrix} \quad \text{or}\quad \begin{bmatrix}
1 & 1\\
0 & 1
\end{bmatrix}.\]
As shown above, in the first case $A$ is the image $\text{exp}(P^{-1}T_1P)$, where 
\[ T_1 = \begin{bmatrix}
	\log\lambda_1 & 0 \\
	0 & -\log\lambda_1
\end{bmatrix},\]
while in the second case $A = \text{exp}(P^{-1}T_2P)$, where
\[T_2 = \begin{bmatrix}
	0 & 1 \\
	0 & 0
\end{bmatrix}.\]
Thus the image of the exponential map on $\mathfrak{sl}(2, \R)$ is exactly those matrices with determinant $1$ and trace $\geq 2$.  
\end{proof}
\begin{problem}{6}
	Br\"ocker \& tom Dieck, I.3.13.2: Show that the exponential map is surjective for $\text{SO}(n)$ and $\text{U}(n)$.	
\end{problem}
\begin{proof}
	First, we look at the even-dimensional case for $\text{SO}(n)$. let $A \in \text{SO}(2n)$ be an arbitrary orthogonal matrix with determinant $1$. We know that $A = P^{-1}DP$ is conjugate to a block-matrix $D$ of the form 
	\[ \begin{bmatrix}R_1 & 0 & \dots & 0 \\
	  0 & R_2 & \dots & 0 \\
	  \vdots & \vdots & \ddots & \vdots \\
	  0 & 0 &   \dots & R_n \end{bmatrix}, \]
	  Where $R_i = \begin{bmatrix}
		  \cos \theta_i & - \sin \theta_i \\
		  \sin \theta_i & \cos \theta_i
	  \end{bmatrix}$
	  Now, as shown in \cite{lax}, each $R_i = \exp(S_i)$, where
	  \[S_i = \begin{bmatrix}
		  0 & -\theta_i \\
		  \theta_i & 0
  \end{bmatrix}.\]
  Because exponentiation commutes with conjugation and with the block-diagonal construction, we see that
  \begin{align*}
	  A &= P^{-1}DP \\
	  &= P^{-1}\begin{bmatrix}
		  R_1 & 0 & \dots & 0\\
		  0 & R_2 & \dots & 0\\
		  \vdots & \vdots & \ddots & \vdots \\
		  0 & 0 & \dots & R_n
	  \end{bmatrix} P\\
	  &= P^{-1}\begin{bmatrix}
		  \exp S_1 & 0 & \dots & 0\\
		  0 & \exp S_2 & \dots & 0 \\
		  \vdots & \vdots & \ddots & \vdots \\
		  0 & 0 & \dots & \exp S_n
	  \end{bmatrix}P\\
	  &= P^{-1}\left (\exp \begin{bmatrix}
		  S_1 & 0 & \dots & 0\\
		  0 & S_2 & \dots & 0 \\
		  \vdots & \vdots & \ddots & \vdots \\
		  0 & 0 & \dots & S_n
  \end{bmatrix}\right )P\\
	  &= \exp \left (P^{-1}\begin{bmatrix}
		  S_1 & 0 & \dots & 0\\
		  0 & S_2 & \dots & 0 \\
		  \vdots & \vdots & \ddots & \vdots \\
		  0 & 0 & \dots & S_n
	  \end{bmatrix}P\right )
  \end{align*}
  The matrix within the exponential is matrix is skew-symmetric, because each $S_i$ is, so it is a member of $\mathfrak{so}(2n)$. Therefore, every element of $\text{SO}(2n)$ is in the image of $\exp$.
  \par We can expand this result in two directions: by looking at $\text{SO}(2n)$ as a subgroup of $\text{SO}(2n + 1)$, and at the subgroup $\U(n)$ of $\text{SO}(2n)$. 
  \par We first look at $\text{SO}(2n + 1)$. It is shown in \cite{lax} that every matrix $A$ in $\tex{SO}(2n + 1)$ is conjugate, as $A = P^{-1}DP$,  to one of the form $D$, where 
  \[D = \begin{bmatrix}
		  R_1 & 0 & \dots & 0 & 0\\
		  0 & R_2 & \dots & 0 & 0\\
		  \vdots & \vdots & \ddots & \vdots & \vdots\\
		  0 & 0 & \dots & R_n & 0\\
		  0 & 0 & \dots & 0 & 1
  \end{bmatrix},\]
  where each $R_i$ is again a $2\times 2$ rotation matrix. This, too, is in the image of $\exp$: letting $S_i = \begin{bmatrix}
	  0 & -\theta \\ \theta & 0
  \end{bmatrix}$, so that $R_i = \exp S_i$, we see that
  \begin{align*}
	  A = \exp \left( P^{-1} \begin{bmatrix}
		  S_1 & 0 & \dots & 0 & 0\\
		  0 & S_2 & \dots & 0 & 0\\
		  \vdots & \vdots & \ddots & \vdots & \vdots\\
		  0 & 0 & \dots & S_n & 0\\
		  0 & 0 & \dots & 0 & 0
  \end{bmatrix}P\right) 
  \end{align*}
  Again, the matrix inside the exponential is skew-symmetric, so it is in $\mathfrak{so}(2n + 1)$.
  \par In the case $A \in \text{U}(n)$, it is shown in \cite{lax} that $A$ is similar to a diagonal matrix, $A = P^{-1}DP$, where the diagonal entries $d_{jj}$ of $D$ are all complex numbers with norm $1$. We may write $d_{jj} = e^{i\theta_j}$, where $\theta_j \in [0, 2\pi)$ is the argument of $d_{jj}$. Therefore, $D$ is the exponential of the diagonal matrix $H$, where $h_{jj} = i\theta_j$, and, further, $H$ is skew-hermitian, because $-\overline{h_{jj}} = -(-i\theta_j) = i\theta_j$, and so $-H^\dagger = H$. So, we see that
  \[A = \exp \left( P^{-1}HP \right),\]
  where $P^{-1}HP$ is skew-Hermitian, and therefore an element of $\mathfrak{u}(n)$. Therefore $\exp$ is surjective on the groups $\text{SO}(2n)$, $\text{SO}(2n + 1)$, and $\text{U}(n)$.
\end{proof}
\begin{problem}{7}
	Br\"ocker \& tom Dieck, I.3.13.6: Show that a compact connected holomorphic Lie group is abelian, i.e., it is isomorphic to $\C^n/B$, where $B$ is a discrete subgroup of $\C^n$. 
\end{problem}
\begin{proof}
	Let $G$ be a compact, connected, holomorphic Lie group. Given any $g$ in $G$, let $c(g)$ be the function $G \to G$ taking $x$ to its adjunction by $g$, $gxg^{-1}$, and $L$ is the differential map. Because $c$ is the composition of multiplications and inversions, which are assumed to be holomorphic on $G$, the map $c$ itself is a holomorphic function on the compact manifold $G$. All holomorphic functions on compact manifolds are locally constant. $G$ is connected, so $c$ is a constant. $c(e)$ is the identity map $x \to exe^{-1}$, so $c(g)$ must be the identity map for all $g$ - that is, $gxg^{-1} = x$ for all $x$. 
	\par As shown in \cite{brock}, for any abelian Lie group $G$, the image of the exponential map is a homomorphism. Because the image $\text{exp}$ contains a neighborhood of the origin, which generates the connected component of the origin, $\text{exp}$ must be a surjective group homomorphism for any connected abelian Lie group.
	\par Because $\text{exp}$ is a local bijection near the origin, the kernel $K$ of $\text{exp}$ must be a discrete subgroup of $LG$; this is because the kernel of a local bijection must be locally trivial, and a nondiscrete subgroup would, by translation to the origin, intersect any neighborhood of the origin at more than one point.
	\par By lemma 3.8 in \cite{brock}, any discrete subgroup $K$ of the finite-dimensional vector space $LG$ is generated by linearly independent vectors $g_1, \dots g_k$. Choosing a set $g_{k+1}, g_{n}$ to complete the basis of $LG$, this gives an isomorphism $LG \cong \C^n$ under which $K$ is isomorphic to a discrete subgroup $B$ of $\C^n$, giving $G \cong \C^{n} / B$ as desired.
\end{proof}
\end{section}
\begin{section}{Problems from the course website}
\begin{problem}{1*}
	Let $G$ be a locally compact, 2nd countable, topological group and $H$ a closed subgroup of $G$. Show that $G/H$ is locally compact, 2nd countable, Hausdorff.
\end{problem}
\begin{proof}
	
\end{proof}
\begin{problem}{2*}
	Let $(G,m,i)$ be a group such that $G$ is a $C^k$ manifold, for $k \in \Z_{>0} \cup \left\{ \infty, \omega \right\}$, and $m$ is $C^k$-map. Show 
	\begin{enumerate}[label=(\alph*)]
		\item $f: G \times G \to G \times G, f(x,y) = (x, xy)$ is a bijective $C^k$ map.
		\item $f_{*, (x,y)}$ is bijective for all $(x,y) \in G \times G$
		\item $f$ is a global $C^k$ diffeomorphism.
		\item $i$ is a $C^k$ map.
	\end{enumerate}
\end{problem}
\begin{proof}
	
\end{proof}
\begin{problem}{3}
	Let $G$ be a connected, abelian Lie group. Show that:
	\begin{enumerate}[label=(\alph*)]
		\item $\text{exp}$ is a local diffeomorphism everywhere.
		\item $\Lambda := \text{exp}^{-1}(1)$ is a discrete subgroup of $(\mathfrak g, +)$
		\item $G$ and $\mathfrak g / \Lambda$ are isomorphic as Lie groups.
	\end{enumerate}
\end{problem}
\begin{proof}
	\begin{enumerate}[label=(\alph*)]
		\item $\exp$ is a local diffeomorphism of some neighborhood $U$ of $\mathfrak g$ to some neighborhood $U'$ of $G$ at the origin; this is by the inverse function theorem and the fact that its differential at $0$ is the identity, which is invertible. Let $U'$ be some neighborhood of $0$ on which $\exp$ is a diffeomorphism, and let $U = \exp(U')$. Now, let $g, W'$ be some element $g \in \mathfrak g$; let $h = \exp(g)$, $l_h$ the left-multiplication function by $h$, and $f_g^{-1}$ the function taking an element $x \in \mathfrak g$ to $x - \mathfrak g$. Then $l_h, f_g$, and $\exp\lvert_{U'}$ are diffeomorphisms on their domains, making the function $l_h \circ \exp \circ f_g^{-1}$ a diffeomorphism from the neighborhood $U'+g$ of $g$ to the neighborhood $l_h(U)$ of $h$. We now need only show that $l_h \circ \exp \circ f_g^{-1} = \exp\lvet_{U'+g}$, i.e. that the following diagram commutes:
			\[\begin{tikzcd}&U' \arrow[r, "\exp"]  &U \arrow[d, "l_h"]\\&U'+g  \arrow[u, "f_g^{-1}"]\arrow[r, "\exp"] & l_hU\end{tikzcd}\]
			This follows from the fact that $G$ is abelian, and therefore $\exp : \mathfrak g \to G$ is a homomorphism of Lie groups. Given any $x \in U' + g$, we see:
			\begin{align*}
				(l_h \circ \exp \circ f_g^{-1})(x) &= h\exp(x-g)\\
				&= \exp(g)\exp(x-g)\\
				&= \exp(g + x - g)\\
				&= \exp(x)
			\end{align*}
			Therefore, the local diffeomorphism $l_h \circ \exp \circ f_g$ is equal to $\exp$, and $\exp$ is indeed a local diffeomorphism on the whole of $\mathfrak g$.
		\item Let $\Lambda:= \exp^{-1}(1)$ be the inverse image of the identity of $G$ under the exponential map. We first see that $\Lambda$ is a subgroup of $G$: By the fact that $\exp$ is a homomorphism, we see that if $x \in \Lambda$, then $\exp(-x) = (\exp(x))^{-1} = 1^{-1} = 1$, and if $x, y \in \Lambda$, then $\exp(x + y) = \exp(x)\exp(y) = 1\cdot 1 = 1$.
			\par Now, we show that $\Lambda$ is discrete. By translation to the origin, it suffices to show that there is a neighborhood $U'$ of $0$ in $\mathfrak g$ such that $U' \cap \Lambda = \left\{ 0 \right\}$. The neighborhood we chose earlier, $U'$, suffices, as $\exp\lvert_{U'}:  U' \to U$ is a local diffeomorphism, and therefore a bijection: $\exp\lvert_{U'}^{1}$ must be the singleton $\left\{ 0 \right\}$. By translation, there is an open set around every element of $\Lambda$ which contains no other element of $\Lambda$. Therefore, this subgroup is discrete.
		\item Because $\exp$ is a smooth Lie group homomorphism, and its kernel is $\Lambda$, it can be factored into a surjective map $\mathfrak g \to \mathfrak g / \Lambda$, composed with an injective immersion $\mathfrak g / \Lambda \to G$ which is also a Lie group homomorphism. Further, the image of this immersion contains a neighborhood of the origin, because it is a local diffeomorphism at $e$. Because $G$ is connected, it is generated by any neighborhood of the origin, so the image of this immersion must be $G$ itself. A surjective, injective immersion is a diffeomorphism, so these two Lie groups are diffeomorphic, and this diffeomorphism is also a Lie group isomorphism.
	\end{enumerate}
\end{proof}
\begin{problem}{4*}
	Let $dX$ denote the Lesbesgue measure on $\mathcal{M}(n, \R) \cong \R^{n^2}$. Show that $d\mu := \left \lvert { \det(X) } \right \lvert^{-n}dX$ is a left invariant Haar measure on $\text{GL}(n, \R)$. Compute the modular function for $\text{GL}(n, \R)$.
	
\end{problem}
\begin{proof}
	
\end{proof}
\begin{problem}{5}
	Let $G= \left\{ \begin{pmatrix}
		x&y\\0&z
\end{pmatrix} \mid x, y, z \in \R, xz > 0\right\}$. Find a left invariant Haar measure, a right invariant Haar measure, and the modular function for $G$.
\end{problem}
\begin{proof}
	This proof follows a method to compute the Haar measure of a subgroup of $G$ in the lecture notes found at \cite{jhu}. 
	\\Let $d\left( \begin{bmatrix}
		x&y\\0&z
\end{bmatrix}\right) = \sigma(x,y,z)dxdydz$. For an arbitrary element $\begin{bmatrix}
	a & b\\0 & c
\end{bmatrix}$, since $\begin{bmatrix}
	a&b\\0&c
\end{bmatrix}\begin{bmatrix}
	x&y\\0&z
\end{bmatrix} = \begin{bmatrix}
	ax & ay + bz\\0 & cz
\end{bmatrix}$, invariance of the measure means that
\begin{align*}\sigma(x,y,z)dxdydz &= \sigma(ax, ay+bz, cz)d(ax)d(ay+bz)d(cz)\\
&= \sigma(ax,ay+bz,cz)a^2c\,dxdydz\end{align*}
\end{proof}
\begin{problem}{6}
	$\text{SU}(2)$ acts by conjugation on $\left\{ \xi = \begin{pmatrix}
		x & y + iz\\y-iz & -x
\end{pmatrix} \mid x, y, z \in \R\right\}$ and leaves invariant the positive definite quadratic form $\xi \mapsto -\text{det}(\xi)$. This action induces a Lie group morphism $f : \text{SU}(2) \to \text{SO}(3)$. Compute $f_{*,1}$.
\end{problem}
\begin{proof}
	We compute the map $f_{*,1}: \mathfrak{su}(2) \to \mathfrak{so}(3)$. We use the following basis for $\mathfrak{su}(2)$:
	\[u_1 = \begin{bmatrix}
		0 & i \\ i & 0
	\end{bmatrix}, \; u_2 = \begin{bmatrix}
	0 & -1 \\1 & 0	
\end{bmatrix}, \; u_3 = \begin{bmatrix}
i & 0 \\0 & -i
\end{bmatrix}\]
And we use the following basis for $\mathfrak{so}(3)$:
\[L_x = \begin{bmatrix}
	0 & 0 & 0 \\ 0 & 0 & -1 \\0 & 1 & 0
\end{bmatrix}, \; L_y = \begin{bmatrix}
	0 & 0 & 1 \\ 0 & 0 & 0 \\-1 & 0 & 0
\end{bmatrix}, \; L_z = \begin{bmatrix}
0 & -1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 &0
\end{bmatrix}\]
Now, we know that $f$ is a Lie group morphism, and is therefore differentiable. We can perturb the identity matrix by an infinitesimal amount $\delta u_i$ in the direction of each basis vector and look at how it acts on a generic element $(x,y,z)$ of $\R^3$ by conjugation:
\begin{align*}
	(I + \delta u_1)\begin{pmatrix}
		x & y + iz \\y-iz & -x	
	\end{pmatrix}(I+\delta u_1)^*
	&= \begin{pmatrix}
		1 & \delta i\\ \delta i & 1
	\end{pmatrix} \begin{pmatrix}
		x & y + iz \\y-iz & -x	
	\end{pmatrix} \begin{pmatrix}
		1 & -\delta i\\ -\delta i & 1
	\end{pmatrix}\\
	&= \begin{pmatrix}x + \delta y + \delta z & y + zi + \delta xi\\
	\delta xi + y - zi & \delta yi - \delta z - x \end{pmatrix}\begin{pmatrix}
		1 & -\delta i\\ -\delta i & 1
	\end{pmatrix}\\
	&= \begin{pmatrix}
		x + 2\delta z + f_1(\delta^2) & y + iz -2\delta xi + f_2(\delta^2)\\
		y - iz  + 2\delta xi + f_3(\delta^2)& -x  - 2\delta z + f_4(\delta^2)
	\end{pmatrix}\\
	&= \begin{pmatrix}
		x & y + iz \\
		y - iz & -x
	\end{pmatrix} + 2\delta z\begin{pmatrix}
		1 & 0 \\ 0 & -1
	\end{pmatrix} + 2\delta x \begin{pmatrix}
		0 & -1 \\ 1 & 0	
	\end{pmatrix} + f(\delta^2)\\
	&= (x,y,z) + 2\delta (z, 0, -x)\\
	&= I(x,y,z) + 2\delta \begin{pmatrix}
		0 & 0 & 1\\
		0 & 0 & 0\\
		-1 & 0 & 0
	\end{pmatrix}(x,y,z)\\
	&= ( I + 2\delta L_y)(x,y,z)
\end{align*}
So the partial derivative in the direction of $u_1$ is $2L_y$. Running the same calculations for $I + \delta u_2$:
\begin{align*}
	(I + \delta u_2) \begin{pmatrix}
		x & y + iz \\ y - iz & -x		
	\end{pmatrix}(I + \delta u_2)^* &= \begin{pmatrix}
		1 & -\delta\\ \delta & 1
	\end{pmatrix}\begin{pmatrix}
		x & y + iz \\ y - iz & -x		
	\end{pmatrix} \begin{pmatrix}1 & \delta\\-\delta & 1\end{pmatrix}\\
	&= \begin{pmatrix}
		x - \delta y + \delta  i z & y + iz  + \delta x \\
		\delta x + y - iz & \delta y + \delta iz - x
	\end{pmatrix}\begin{pmatrix}
	1 & \delta \\ -\delta & 1
	\end{pmatrix}\\
	&= \begin{pmatrix}
		x - 2 \delta y + f_1(\delta^2) & y + iz + 2\delta x + f_2(\delta^2)\\
		y - iz + 2\delta x + f_3(\delta^2) &  i-x + 2 \delta y + f_4(\delta^2)
	\end{pmatrix}\\
	&= \begin{pmatrix}
		x & y + iz \\
		y - iz & -x
	\end{pmatrix} - 2\delta y \begin{pmatrix}
		1 & 0 \\ 0 & -1
	\end{pmatrix} + 2 \delta x \begin{pmatrix}
		0 & 1 \\1 & 0
	\end{pmatrix}\\
	&= (x,y,z) + 2\delta (-y, x, 0)\\
	&= I (x,y,z) + 2\delta \begin{pmatrix}
		0 & -1 & 0 \\
		1 & 0 & 0\\
		0 & 0 & 0
	\end{pmatrix}\\
	&= (I + 2\delta L_z)(x,y,z)
\end{align*} 
Therefore, the partial derivative in the direction of $u_2$ is $2L_z$. Finally, we run through these calculations for $I + \delta u_3$:
\begin{align*}
	(I + \delta u_3)\begin{pmatrix}
		x & y + iz \\
		y - iz & x
	\end{pmatrix}(I + \delta u_3)* &= \begin{pmatrix}
		1 + \delta i & 0 \\
		0 & 1 - \delta i
	\end{pmatrix} \begin{pmatrix}
		x & y + iz \\
		y - iz & -x
	\end{pmatrix}\begin{pmatrix}
		1 - \delta i & 0 \\
		0 & 1 + \delta i
	\end{pmatrix}\\
	&= \begin{pmatrix}
		x + \delta x i & y + iz + \delta y i - \delta z\\
		y - zi - \delta y i - \delta z & -x + \delta x i 
	\end{pmatrix}\begin{pmatrix}
		1 - \delta i & 0 \\ 0 & 1 + \delta i
	\end{pmatrix}\\
	&= \begin{pmatrix}
		x + f_1(\delta^2) & y + iz + 2 \delta y i - 2 \delta z + f_2(\delta^2)\\
		y - iz - 2 \delta y i - 2 \delta z   + f_3(\delta^2)& -x + f_4(\delta^2)
	\end{pmatrix}\\
	&= \begin{pmatrix}
		x & y + iz\\
		y - iz & -x
	\end{pmatrix} - 2\delta z\begin{pmatrix}
		0 & 1\\
		1 & 0
	\end{pmatrix} + 2\delta y \begin{pmatrix}
		0 & i\\
		-i & 0
	\end{pmatrix}\\
	&= (x,y,z) + 2\delta(0,-z,y)\\
	&= I(x,y,z) + 2\delta \begin{pmatrix}
		0 & 0 & 0\\ 
		0 & 0 & -1\\
	0 & 1 & 0
\end{pmatrix}(x,y,z)\\
&= (I + 2\delta L_x)(x,y,z)
\end{align*}
Therefore, the partial derivative in the direction of $u_3$ is $2L_x$. These three partial derivatives determine the linear map $f_{*,1} : \mathfrak{su}(2) \to \mathfrak{so}(3)$; in the ordered bases we have chosen, 
\[f_{*,1} = \begin{bmatrix}
	0 & 2 & 0\\
	0 & 0 & 2\\
	2 & 0 & 0
\end{bmatrix}\]
\end{proof}
\end{section}
\begin{thebibliography}{}
	\bibitem{brock}{Br\"ocker, T. and tom Dieck, T. Respresentations of Compact Lie Groups. Springer-Verlag, 1985.}
\bibitem{lax}{Lax, Peter D. Linear Algebra and Its Applications. Wiley, 2007. }
\end{thebibliography}
\end{document}
